{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-28T20:57:31.041579Z",
     "start_time": "2024-06-28T20:57:30.738191Z"
    }
   },
   "source": "from pyspark.sql.types import *",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T21:02:20.783823Z",
     "start_time": "2024-06-28T21:02:20.779805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "people_schema = ([\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"city\", StringType(), False),\n",
    "])"
   ],
   "id": "1ffcfb24b8000e2b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T08:43:07.864939Z",
     "start_time": "2024-06-29T08:43:07.854300Z"
    }
   },
   "cell_type": "code",
   "source": "import pyspark.sql.functions as F",
   "id": "454fc0a93e160179",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Examples column string transformation\n",
    "\n",
    "# Apply per columns as transformation\n",
    "voter_df.withColumn(\"upper\", F.upper(\"name\"))\n",
    "\n",
    "# Can create intermediary colmns\n",
    "voter_df.withColumn(\"splits\", F.split(\"name\", \"\"))\n",
    "\n",
    "# Cast other types\n",
    "voter_df.withColumn(\"year\", voter_df[\"_c4\"].cast(IntegerType))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select(\"VOTER_NAME\").distinct().show(40, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains(\"_\"))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select(\"VOTER_NAME\").distinct().show(40, truncate=False)\n",
    "\n",
    "\n",
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn(\"splits\", F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn(\"first_name\", voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn(\"last_name\", voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()\n",
    "\n",
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               when(voter_df.TITLE == \"Councilmember\", F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()\n",
    "\n",
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == \"Mayor\", 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()\n",
    "#voter_df.filter(F.col(\"random_val\") == 0).show()\n",
    "\n",
    "# Using functions in pyspark\n",
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names)\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show()\n",
    "\n"
   ],
   "id": "39b6cce1db94a5c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID using monotonically_increasing_id\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ],
   "id": "a8d51245b491ac7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
    "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_single = voter_df_single.withColumn(\"ROW_ID\", F.monotonically_increasing_id())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
   ],
   "id": "96f11351c6586f6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_id = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_id)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "# Preatention when use column string because \"ROW_ID\" != 'ROW_ID'\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ],
   "id": "f0b91e7cf5ed26d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Using cache in pyspark\n",
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))\n"
   ],
   "id": "4f05f3eb7b1cee46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reading files splitting \n",
    "full_df = spark.read.csv('departures_full.txt.gz')\n",
    "split_df = spark.read.csv(\"departures_*.txt.gz\")\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ],
   "id": "75a0462e4fc86eea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clustering sizing tips\n",
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get(\"spark.app.name\")\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get(\"spark.driver.port\")\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)\n",
    "\n",
    "\n",
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('departures.txt.gz').distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ],
   "id": "8954859e88ad8378"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " # Using explain in pyspark\n",
    "\n",
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()\n"
   ],
   "id": "84b1ae9e51df6555"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ],
   "id": "6ae056b45c33b8ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))\n"
   ],
   "id": "e87f0e1ef3ddd586"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# quick pipeline\n",
    "# Import the data to a DataFrame\n",
    "departures_df = spark.read.csv(\"2015-departures.csv.gz\", header=True)\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.drop(departures_df['Actual elapsed time (Minutes)'] == 0)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "departures_df.write.json(\"output.json\", mode='overwrite')\n"
   ],
   "id": "72cab580b81b8fb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
   ],
   "id": "459727d569ae5bdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'],'\\t')\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (F.size(tmp_fields) < 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
   ],
   "id": "6c47beac2d5a8c80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)"
   ],
   "id": "ada15671c1ede40d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def retriever(cols, colcount):\n",
    "  # Return a list of dog data\n",
    "  return cols[4:colcount]\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Create a new column using your UDF\n",
    "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
    "\n",
    "# Remove the original column, split_cols, and the colcount\n",
    "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')"
   ],
   "id": "ed2e91a26c7b270b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
   ],
   "id": "a7608749f30ac7a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Determine the row counts for each DataFrame\n",
    "split_count = split_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'outer')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# Determine the number of distinct folder rows removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
   ],
   "id": "997c95dc3ddc532c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select the dog details and show 10 untruncated rows\n",
    "print(joined_df.select('dog_list').show(10, truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "\tStructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "  dogs = []\n",
    "  for dog in doglist:\n",
    "    (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "    dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "  return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs and drop the old column\n",
    "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "\n",
    "# Show the number of dogs in the first 10 rows\n",
    "joined_df.select(F.size('dogs')).show(10)"
   ],
   "id": "dc5f76e1ce17a939"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define a UDF to determine the number of pixels per image\n",
    "def dogPixelCount(doglist):\n",
    "  totalpixels = 0\n",
    "  for dog in doglist:\n",
    "    totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "  return totalpixels\n",
    "\n",
    "# Define a UDF for the pixel count\n",
    "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
    "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
    "\n",
    "# Create a column representing the percentage of pixels\n",
    "joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100)\n",
    "\n",
    "# Show the first 10 annotations with more than 60% dog\n",
    "joined_df.filter(joined_df['dog_percent'] > 60).show(10)"
   ],
   "id": "3f12e50a073e7d91"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
